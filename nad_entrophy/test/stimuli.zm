/******************************************************************************\
FILE:           stimuli.zm
AUTHOR:         Theo Veenker <theo.veenker@beexy.nl>
ADAPTED BY:     Silvia Radulescu
LAST MODIFIED:	2015-03-17

DESCRIPTION:

Defines the formats of item table(s) and provides the actual content of
the item table(s) i.e. stimulus information.


HISTORY:
2011-10-25 TV   Created.

\******************************************************************************/

import condition;

// Symbolic definition of item type.
enum Type
{
    TRAIN_LOW,               // training string low entropy
    TRAIN_HIGH,              // training string high entropy
    TYPE1,                   // test string type1 - XXY trained
    TYPE2,                   // test string type2 - X1X2Y trained
    TYPE3,                   // test string type3 - XXY untrained
    TYPE4,                   // test string type4 - X1X2Y untrained
    CHECKIN,                 // checkin test
}


// Item table record.
record TestItem
{
    int         id;             // item id
    int         nth_rep;        // the number of repetition.
    CondEntropy entropy;        // entropy of item
    CondLang    language;       // L1 or L2
    string      sndfn;          // stimulus word sound filename
}

record CheckinItem
{
    int           id;
    Type          type;
    string        value;
}


// Training items of language 1 with low entropy
TestItem[] items_low_l1 =
{
    {1, 0, EN_LOW, LANG_L1, "rak_bap_toef"},
    {2, 0, EN_LOW, LANG_L1, "rak_gom_toef"},
    {3, 0, EN_LOW, LANG_L1, "rak_ves_toef"},
    {4, 0, EN_LOW, LANG_L1, "sot_bap_jik"},
    {5, 0, EN_LOW, LANG_L1, "sot_gom_jik"},
    {6, 0, EN_LOW, LANG_L1, "sot_ves_jik"},
    {7, 0, EN_LOW, LANG_L1, "tep_bap_lut"},
    {8, 0, EN_LOW, LANG_L1, "tep_gom_lut"},
    {9, 0, EN_LOW, LANG_L1, "tep_ves_lut"},
};

// Training items of language 1 with high entropy
TestItem[] items_high_l1 = {
    {1, 0,  EN_HIGH, LANG_L1, "rak_bap_toef"},
    {2, 0,  EN_HIGH, LANG_L1, "rak_gom_toef"},
    {3, 0,  EN_HIGH, LANG_L1, "rak_ves_toef"},
    {4, 0,  EN_HIGH, LANG_L1, "sot_bap_jik"},
    {5, 0,  EN_HIGH, LANG_L1, "sot_gom_jik"},
    {6, 0,  EN_HIGH, LANG_L1, "sot_ves_jik"},
    {7, 0,  EN_HIGH, LANG_L1, "tep_bap_lut"},
    {8, 0,  EN_HIGH, LANG_L1, "tep_gom_lut"},
    {9, 0,  EN_HIGH, LANG_L1, "tep_ves_lut"},
    {10, 0, EN_HIGH, LANG_L1, "rak_dos_toef"},
    {11, 0, EN_HIGH, LANG_L1, "rak_huf_toef"},
    {12, 0, EN_HIGH, LANG_L1, "rak_mip_toef"},
    {13, 0, EN_HIGH, LANG_L1, "rak_nit_toef"},
    {14, 0, EN_HIGH, LANG_L1, "rak_pem_toef"},
    {15, 0, EN_HIGH, LANG_L1, "rak_wop_toef"},
    {16, 0, EN_HIGH, LANG_L1, "sot_dos_jik"},
    {17, 0, EN_HIGH, LANG_L1, "sot_huf_jik"},
    {18, 0, EN_HIGH, LANG_L1, "sot_mip_jik"},
    {19, 0, EN_HIGH, LANG_L1, "sot_nit_jik"},
    {20, 0, EN_HIGH, LANG_L1, "sot_pem_jik"},
    {21, 0, EN_HIGH, LANG_L1, "sot_wop_jik"},
    {22, 0, EN_HIGH, LANG_L1, "tep_dos_lut"},
    {23, 0, EN_HIGH, LANG_L1, "tep_huf_lut"},
    {24, 0, EN_HIGH, LANG_L1, "tep_mip_lut"},
    {25, 0, EN_HIGH, LANG_L1, "tep_nit_lut"},
    {26, 0, EN_HIGH, LANG_L1, "tep_pem_lut"},
    {27, 0, EN_HIGH, LANG_L1, "tep_wop_lut"},
    {28, 0, EN_HIGH, LANG_L1, "rak_bif_toef"},
    {29, 0, EN_HIGH, LANG_L1, "rak_dul_toef"},
    {30, 0, EN_HIGH, LANG_L1, "rak_hog_toef"},
    {31, 0, EN_HIGH, LANG_L1, "rak_jal_toef"},
    {32, 0, EN_HIGH, LANG_L1, "rak_keg_toef"},
    {33, 0, EN_HIGH, LANG_L1, "rak_kof_toef"},
    {34, 0, EN_HIGH, LANG_L1, "rak_naf_toef"},
    {35, 0, EN_HIGH, LANG_L1, "rak_nup_toef"},
    {36, 0, EN_HIGH, LANG_L1, "rak_zuk_toef"},
    {37, 0, EN_HIGH, LANG_L1, "sot_bif_jik"},
    {38, 0, EN_HIGH, LANG_L1, "sot_dul_jik"},
    {39, 0, EN_HIGH, LANG_L1, "sot_hog_jik"},
    {40, 0, EN_HIGH, LANG_L1, "sot_jal_jik"},
    {41, 0, EN_HIGH, LANG_L1, "sot_keg_jik"},
    {42, 0, EN_HIGH, LANG_L1, "sot_kof_jik"},
    {43, 0, EN_HIGH, LANG_L1, "sot_naf_jik"},
    {44, 0, EN_HIGH, LANG_L1, "sot_nup_jik"},
    {45, 0, EN_HIGH, LANG_L1, "sot_zuk_jik"},
    {46, 0, EN_HIGH, LANG_L1, "tep_bif_lut"},
    {47, 0, EN_HIGH, LANG_L1, "tep_dul_lut"},
    {48, 0, EN_HIGH, LANG_L1, "tep_hog_lut"},
    {49, 0, EN_HIGH, LANG_L1, "tep_jal_lut"},
    {50, 0, EN_HIGH, LANG_L1, "tep_keg_lut"},
    {51, 0, EN_HIGH, LANG_L1, "tep_kof_lut"},
    {52, 0, EN_HIGH, LANG_L1, "tep_naf_lut"},
    {53, 0, EN_HIGH, LANG_L1, "tep_nup_lut"},
    {54, 0, EN_HIGH, LANG_L1, "tep_zuk_lut"}
}

// Training items of language 2 with low entropy
TestItem[] items_low_l2 =
{
    {1, 0, EN_LOW, LANG_L2, "rak_bap_lut"},
    {2, 0, EN_LOW, LANG_L2, "rak_gom_lut"},
    {3, 0, EN_LOW, LANG_L2, "rak_ves_lut"},
    {4, 0, EN_LOW, LANG_L2, "sot_bap_toef"},
    {5, 0, EN_LOW, LANG_L2, "sot_gom_toef"},
    {6, 0, EN_LOW, LANG_L2, "sot_ves_toef"},
    {7, 0, EN_LOW, LANG_L2, "tep_bap_jik"},
    {8, 0, EN_LOW, LANG_L2, "tep_gom_jik"},
    {9, 0, EN_LOW, LANG_L2, "tep_ves_jik"},
};


// Training items of language 2 with high entropy
TestItem[] items_high_l2 = {
    {1, 0,  EN_HIGH, LANG_L2, "rak_bap_lut"},
    {2, 0,  EN_HIGH, LANG_L2, "rak_gom_lut"},
    {3, 0,  EN_HIGH, LANG_L2, "rak_ves_lut"},
    {4, 0,  EN_HIGH, LANG_L2, "sot_bap_toef"},
    {5, 0,  EN_HIGH, LANG_L2, "sot_gom_toef"},
    {6, 0,  EN_HIGH, LANG_L2, "sot_ves_toef"},
    {7, 0,  EN_HIGH, LANG_L2, "tep_bap_jik"},
    {8, 0,  EN_HIGH, LANG_L2, "tep_gom_jik"},
    {9, 0,  EN_HIGH, LANG_L2, "tep_ves_jik"},
    {10, 0, EN_HIGH, LANG_L2, "rak_dos_lut"},
    {11, 0, EN_HIGH, LANG_L2, "rak_huf_lut"},
    {12, 0, EN_HIGH, LANG_L2, "rak_mip_lut"},
    {13, 0, EN_HIGH, LANG_L2, "rak_nit_lut"},
    {14, 0, EN_HIGH, LANG_L2, "rak_pem_lut"},
    {15, 0, EN_HIGH, LANG_L2, "rak_wop_lut"},
    {16, 0, EN_HIGH, LANG_L2, "sot_dos_toef"},
    {17, 0, EN_HIGH, LANG_L2, "sot_huf_toef"},
    {18, 0, EN_HIGH, LANG_L2, "sot_mip_toef"},
    {19, 0, EN_HIGH, LANG_L2, "sot_nit_toef"},
    {20, 0, EN_HIGH, LANG_L2, "sot_pem_toef"},
    {21, 0, EN_HIGH, LANG_L2, "sot_wop_toef"},
    {22, 0, EN_HIGH, LANG_L2, "tep_dos_jik"},
    {23, 0, EN_HIGH, LANG_L2, "tep_huf_jik"},
    {24, 0, EN_HIGH, LANG_L2, "tep_mip_jik"},
    {25, 0, EN_HIGH, LANG_L2, "tep_nit_jik"},
    {26, 0, EN_HIGH, LANG_L2, "tep_pem_jik"},
    {27, 0, EN_HIGH, LANG_L2, "tep_wop_jik"},
    {28, 0, EN_HIGH, LANG_L2, "rak_bif_lut"},
    {29, 0, EN_HIGH, LANG_L2, "rak_dul_lut"},
    {30, 0, EN_HIGH, LANG_L2, "rak_hog_lut"},
    {31, 0, EN_HIGH, LANG_L2, "rak_jal_lut"},
    {32, 0, EN_HIGH, LANG_L2, "rak_keg_lut"},
    {33, 0, EN_HIGH, LANG_L2, "rak_kof_lut"},
    {34, 0, EN_HIGH, LANG_L2, "rak_naf_lut"},
    {35, 0, EN_HIGH, LANG_L2, "rak_nup_lut"},
    {36, 0, EN_HIGH, LANG_L2, "rak_zuk_lut"},
    {37, 0, EN_HIGH, LANG_L2, "sot_bif_toef"},
    {38, 0, EN_HIGH, LANG_L2, "sot_dul_toef"},
    {39, 0, EN_HIGH, LANG_L2, "sot_hog_toef"},
    {40, 0, EN_HIGH, LANG_L2, "sot_jal_toef"},
    {41, 0, EN_HIGH, LANG_L2, "sot_keg_toef"},
    {42, 0, EN_HIGH, LANG_L2, "sot_kof_toef"},
    {43, 0, EN_HIGH, LANG_L2, "sot_naf_toef"},
    {44, 0, EN_HIGH, LANG_L2, "sot_nup_toef"},
    {45, 0, EN_HIGH, LANG_L2, "sot_zuk_toef"},
    {46, 0, EN_HIGH, LANG_L2, "tep_bif_jik"},
    {47, 0, EN_HIGH, LANG_L2, "tep_dul_jik"},
    {48, 0, EN_HIGH, LANG_L2, "tep_hog_jik"},
    {49, 0, EN_HIGH, LANG_L2, "tep_jal_jik"},
    {50, 0, EN_HIGH, LANG_L2, "tep_keg_jik"},
    {51, 0, EN_HIGH, LANG_L2, "tep_kof_jik"},
    {52, 0, EN_HIGH, LANG_L2, "tep_naf_jik"},
    {53, 0, EN_HIGH, LANG_L2, "tep_nup_jik"},
    {54, 0, EN_HIGH, LANG_L2, "tep_zuk_jik"}
}


TestItem[] training_items1 = {};
TestItem[] training_items2 = {};
TestItem[] training_items3 = {};

// Test item table.
//   index: item number
TestItem[..][..] test_items =
{
    { // TEST1
//        {1,         TYPE1,           "daa_daa_lie.wav"},
//        {2,         TYPE2,           "joe_daa_saa.wav"},
//        {3,         TYPE3,           "duu_duu_taa.wav"},
//        {4,         TYPE4,           "poo_gaa_roe.wav"},
    },
    { // TEST2
//        {5,         TYPE1,           "hie_hie_saa.wav"},
//        {6,         TYPE2,           "puu_teu_muu.wav"},
//        {7,         TYPE3,           "zoe_zoe_voo.wav"},
//        {8,         TYPE4,           "roe_nuu_nie.wav"},
    },
    { // TEST3
//        {9,         TYPE1,           "kee_kee_muu.wav"},
//        {10,        TYPE2,           "kee_foo_vee.wav"},
//        {11,        TYPE3,           "soo_soo_ruu.wav"},
//        {12,        TYPE4,           "gaa_mie_suu.wav"},
    },
    { // TEST4
//        {13,        TYPE1,           "teu_teu_vee.wav"},
//        {14,        TYPE2,           "hie_daa_reu.wav"},
//        {15,        TYPE3,           "jie_jie_feu.wav"},
//        {16,        TYPE4,           "suu_nie_nuu.wav"},
    },
    { // TEST5
//        {17,        TYPE1,           "joe_joe_goo.wav"},
//        {18,        TYPE2,           "teu_puu_goo.wav"},
//        {19,        TYPE3,           "woe_woe_see.wav"},
//        {20,        TYPE4,           "mie_poo_gaa.wav"},
    },
};

void append_test_item(TestItem[] array, TestItem item) {
    array.size = array.size + 1;
    array[array.size - 1] = item;
}

/*
 * Distributes all trainings items over the three separate lists/blocks
 */
void prepare_training_lists(TestItem[] input) {
    TestItem[] master_list = {};
    TestItem[] current_list;
    int nth_rep = 0;
    int i = 0;
    int start;
    int end;
    while (master_list.size < TOT_TRAININGS_ITEMS) {
        i = 0;
        while(i < input.size) {
            TestItem item = input[i];
            item.nth_rep = nth_rep;
            append_test_item(master_list, item);
            i++;
        }
        nth_rep++;
    }
    if (master_list.size != TOT_TRAININGS_ITEMS) {
        print_error("The number of trainings items is " + master_list.size +". "); 
        print_error("We expected it to be " + TOT_TRAININGS_ITEMS + "\n");
    }

    // prepare list 1
    current_list = training_items1;
    start = TOT_TRAININGS_ITEMS / NUM_TRAINING_BLOCKS * 0;
    end = start + NUM_TRAININGS_ITEMS;
    i = start;
    while (i < end) {
        append_test_item(current_list, master_list[i]);
        i++;
    }
    
    // prepare list 2
    current_list = training_items2;
    start = TOT_TRAININGS_ITEMS / NUM_TRAINING_BLOCKS * 1;
    end = start + NUM_TRAININGS_ITEMS;
    i = start;
    while (i < end) {
        append_test_item(current_list, master_list[i]);
        i++;
    }
    
    // prepare list 3
    current_list = training_items3;
    start = TOT_TRAININGS_ITEMS / NUM_TRAINING_BLOCKS * 2;
    end = start + NUM_TRAININGS_ITEMS;
    i = start;
    while (i < end) {
        append_test_item(current_list, master_list[i]);
        i++;
    }
}

/*
 * Selects the right source list for the current conditions and
 * prepares the training lists.
 */
void prepare_training_items() {

    TestItem[] training_items;

    if (chosen_entropy == EN_HIGH) {
        if (chosen_language == LANG_L1)
            training_items = items_high_l1;
        else 
            training_items = items_high_l2;
    }
    else {
        if (chosen_language == LANG_L1)
            training_items = items_low_l1;
        else
            training_items = items_low_l2;
    }

    prepare_training_lists(training_items);
}

